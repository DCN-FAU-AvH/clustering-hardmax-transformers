{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run in case the packages are not installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas, matplotlib, nltk, torch, imageio, tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "from utils.models import TClassifier\n",
    "from utils.training import Trainer\n",
    "from utils.graphics import plot_review\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('./data/imdb_preprocessed.csv')\n",
    "\n",
    "# get all processed reviews\n",
    "reviews = data.processed.values\n",
    "\n",
    "# merge into single variable, separated by whitespaces\n",
    "words = ' '.join(reviews)\n",
    "\n",
    "# obtain list of words\n",
    "words = words.split()\n",
    "\n",
    "# build vocabulary\n",
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "int2word[0] = '<PAD>'\n",
    "word2int = {word: id for id, word in int2word.items()}\n",
    "\n",
    "# encode words\n",
    "reviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n",
    "\n",
    "# padding sequences\n",
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "    padded_count = 0\n",
    "    trimmed_count = 0\n",
    "    \n",
    "    for i, row in enumerate(reviews):\n",
    "        if len(row) > seq_length:\n",
    "            trimmed_count += 1\n",
    "            features[i, :seq_length] = np.array(row)[:seq_length]\n",
    "        else:\n",
    "            padded_count += 1\n",
    "            features[i, :len(row)] = np.array(row)\n",
    "    \n",
    "    return features, padded_count, trimmed_count\n",
    "\n",
    "seq_length = 128\n",
    "features, padded_count, trimmed_count = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)\n",
    "\n",
    "# Assertions to ensure the feature matrix dimensions are as expected\n",
    "assert len(features) == len(reviews_enc)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "print(f\"Number of padded reviews: {padded_count}\")\n",
    "print(f\"Number of trimmed reviews: {trimmed_count}\")\n",
    "\n",
    "# get labels as numpy\n",
    "labels = data.label.to_numpy()\n",
    "\n",
    "# train test split\n",
    "train_size = .7         # we will use 70% of whole data as train set\n",
    "test_size = .3          # and we will use 30% of whole data as test set\n",
    "\n",
    "# make train set and test\n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, test_x = features[:split_id], features[split_id:]\n",
    "train_y, test_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "# define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# create tensor datasets\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "testset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# create dataloaders\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "testloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all hyperparameters for model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "vocab_size = len(word2int)\n",
    "embs = [2, 4, 8, 16]\n",
    "depth = 8 \n",
    "temperature = 0.001\n",
    "lr = 0.001\n",
    "hm_loss = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.BCELoss()  # we use BCELoss for a binary classification problem\n",
    "num_epochs = 100 + 1\n",
    "save_every = 10\n",
    "grad_clip = 5\n",
    "saved_dir = './saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb in embs:\n",
    "    model = TClassifier(vocab_size, emb, seq_length, depth, temperature)\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(model,optim, criterion, device, hm_loss, saved_dir, grad_clip)\n",
    "    trainer.train(trainloader, testloader, num_epochs, emb, save_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot styles\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "markers = ['^', 'o', 's', 'd']\n",
    "linestyles = ['solid','dotted', 'dashed', 'dashdot']\n",
    "\n",
    "# Define plots directory\n",
    "plots_dir = './plots'\n",
    "\n",
    "### Plot of the training losses for both the softmax and hardmax models \n",
    "embs = [2, 4]\n",
    "plt.figure(figsize=(5, 3))\n",
    "for idx, emb in enumerate(embs):\n",
    "    # Load history dictionary from the file\n",
    "    with open(saved_dir + '/history_d=' + str(emb) + '_HM.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    # Plot training loss Softmax\n",
    "    plt.plot(range(history['epochs']), history['train_loss'], label='Softmax $d = $' + str(emb), \n",
    "            color=colors[idx])\n",
    "    # Plot training loss Hardmax\n",
    "    # Filter out the indices and values that are not multiples of 10\n",
    "    ind = [int(i * 10) for i in range(10 + 1)]\n",
    "    values = [history['train_loss_hm'][i] for i in ind]\n",
    "    plt.scatter(ind, values, label='Hardmax $d = $' + str(emb),\n",
    "            marker=markers[idx],\n",
    "            facecolor=\"none\",\n",
    "            s = 50,\n",
    "            color=colors[idx])\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig(plots_dir + '/training_loss_sm_hm' + \".pdf\", format='pdf', dpi=250, bbox_inches='tight')\n",
    "\n",
    "### Plot of the training and test accuracy\n",
    "embs = [2, 4, 8, 16]\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9,2))\n",
    "legend_handles = []  # Store legend handles\n",
    "for idx, emb in enumerate(embs):\n",
    "    # Load history dictionary from the file\n",
    "    with open(saved_dir + '/history_d=' + str(emb) + '.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    # Plot train accuracy\n",
    "    train_line, = axs[0].plot(range(history['epochs']), history['train_acc'], label='$d = $' + str(emb), \n",
    "            linestyle=linestyles[idx],\n",
    "            color=colors[idx])\n",
    "    # Plot test accuracy\n",
    "    axs[1].plot(range(history['epochs']), history['test_acc'], label='$d = $' + str(emb),\n",
    "            linestyle=linestyles[idx],\n",
    "            color=colors[idx])\n",
    "    # Find and print maximum test accuracy and its epoch\n",
    "    max_test_acc = max(history['test_acc'])\n",
    "    max_test_acc_epoch = history['test_acc'].index(max_test_acc)\n",
    "    print(f\"Maximum test accuracy for emb={emb}: {max_test_acc} at epoch {max_test_acc_epoch}\")\n",
    "    # Store handles for legend\n",
    "    legend_handles.append(train_line)\n",
    "\n",
    "# Combine legends into a single legend at the bottom\n",
    "fig.legend(legend_handles, [f'd = {emb}' for emb in embs], loc='upper center',\n",
    "        ncol=len(embs), bbox_to_anchor=(0.5, 1.1))\n",
    "axs[0].set_xlabel('epochs')\n",
    "axs[0].set_ylabel('train accuracy')\n",
    "axs[0].set_ylim(0.5, 1 + 0.05)\n",
    "axs[1].set_xlabel('epochs')\n",
    "axs[1].set_ylabel('test accuracy')\n",
    "axs[1].set_ylim(0.5, 1 + 0.05)\n",
    "\n",
    "plt.savefig(plots_dir + '/acc_embs' + \".pdf\", format='pdf', dpi=250, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table with statistical information about leaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = './saved'\n",
    "embs = [2, 4, 8, 16]\n",
    "N_reviews = np.shape(test_x)[0]\n",
    "leaders = np.zeros((np.size(embs),N_reviews))\n",
    "ini_leaders = np.zeros((np.size(embs),N_reviews))\n",
    "\n",
    "for idx, emb in enumerate(embs):\n",
    "    model = torch.load(saved_dir + '/model_d=' + str(emb) + '_epoch=2.pth')\n",
    "    E = model.state_dict()['encoder.weight'].numpy()\n",
    "    v = model.state_dict()['decoder.weight'].numpy()\n",
    "    b = model.state_dict()['decoder.bias'].numpy()\n",
    "    alpha = model.state_dict()['attention.alpha'].numpy() \n",
    "    X = E[test_x, :] # matrix with the embeddings of all test reviews \n",
    "    _, n, d = np.shape(X)\n",
    "    s1 = 1 / (1 + alpha)\n",
    "    s2 = alpha * s1\n",
    "    for rev in range(N_reviews):\n",
    "        z0 = X[rev,:,:].T #initial configuration embedding\n",
    "        ## Run the hardmax dynamics ##\n",
    "        W = np.zeros((d, n))\n",
    "        z = np.zeros((d, n, depth+1)) #we want to do num_steps number of iterations, +1 to save the initial confiiguration\n",
    "        z[:, :, 0] = z0\n",
    "        f = z0.copy()\n",
    "        for iter in range(depth+1):\n",
    "            for i in range(n):\n",
    "                IP = np.dot(f[:, i],f)\n",
    "                Pij = np.zeros(n)\n",
    "                ind = IP == np.max(IP)\n",
    "                # Get initial leaders (in the layer = 0, i.e. first layer)\n",
    "                if iter == 0:\n",
    "                    if i == np.where(ind)[0][0]:\n",
    "                        ini_leaders[idx,rev] += 1\n",
    "                Pij[ind] = 1. / np.sum(ind)\n",
    "                W[:, i] =  s2 * np.sum(Pij * f, axis=1)\n",
    "            f = s1 * f + W\n",
    "            z[:, :, iter] = f\n",
    "        # Get all leaders (in the layer = depth, i.e. last layer)\n",
    "        for i in range(n):\n",
    "            y = np.dot(z[:, i, depth], z[:, :, depth])\n",
    "            ind = y == np.max(y)\n",
    "            if i == np.where(ind)[0][0]:\n",
    "                leaders[idx,rev] += 1\n",
    "\n",
    "# Define and print a table with statistical information on leaders\n",
    "table = [[\"Average\", *np.mean(leaders, axis = 1)],\n",
    "         [\"Std Dev\", *np.std(leaders, axis = 1)],\n",
    "         [\"Min\", *np.min(leaders, axis = 1)],\n",
    "         [\"Max\", *np.max(leaders, axis = 1)],\n",
    "         [\"Frac. Ini.\", *(np.mean(ini_leaders / leaders, axis = 1))]]\n",
    "\n",
    "print(tabulate(table, headers=embs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select reviews to be plotted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "saved_dir = './saved'\n",
    "model = torch.load(saved_dir + '/model_d=2_epoch=100.pth')\n",
    "\n",
    "# Selected reviews (features) in the test set, those correcty classified with predicted value > 0.999\n",
    "confidence = 0.999 \n",
    "selected_features = []\n",
    "selected_labels = []\n",
    "testloop = tqdm(testloader, leave=True, desc='Inference')\n",
    "with torch.no_grad():\n",
    "    for feature, target in testloop:\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "        out = model(feature)\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        # Select features and labels\n",
    "        confidence_mask = ((out > confidence) | (out < (1 - confidence)))\n",
    "        mask = confidence_mask.squeeze(dim=1) & equals\n",
    "        selected_features.append(feature[mask].cpu())\n",
    "        selected_labels.append(target[mask].cpu())\n",
    "selected_features = torch.cat(selected_features, dim=0)\n",
    "selected_labels = torch.cat(selected_labels, dim=0)\n",
    "print(np.shape(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot evolution of tokens from a single review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = './saved'\n",
    "dir_path = \"./plots\"\n",
    "\n",
    "# Load model\n",
    "model = torch.load(saved_dir + '/model_d=2_epoch=100.pth')\n",
    "\n",
    "# Sect a single review from selected reviews\n",
    "rev_number = 10\n",
    "rev_features = selected_features[rev_number]\n",
    "label = selected_labels[rev_number]\n",
    "\n",
    "# Plotting options\n",
    "options = {\n",
    "    'movie': False,\n",
    "    'save_plots': False,\n",
    "    'mean': True,\n",
    "    'levels': True,\n",
    "    'trail': False,\n",
    "    'start_white': False\n",
    "}\n",
    "plot_review(model, rev_features, label, int2word, depth, dir_path, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the frequency of leaders of correctly classified with high confidence reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "saved_dir = './saved'\n",
    "plots_dir = './plots'\n",
    "emb = 2\n",
    "confidence = 0.95 #0.999\n",
    "selected_features = []\n",
    "selected_labels = []\n",
    "all_leaders = [] #for histogram of leaders\n",
    "testloop = tqdm(testloader, leave=True, desc='Inference')\n",
    "\n",
    "model = torch.load(saved_dir + '/model_d=' + str(emb) + '_epoch=100.pth')\n",
    "with torch.no_grad():\n",
    "    for feature, target in testloop:\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "        out = model(feature)\n",
    "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        equals = predicted == target\n",
    "        # Select features and labels\n",
    "        confidence_mask = ((out > confidence) | (out < (1 - confidence)))\n",
    "        mask = confidence_mask.squeeze(dim=1) & equals\n",
    "        selected_features.append(feature[mask].cpu())\n",
    "        selected_labels.append(target[mask].cpu())\n",
    "selected_features = torch.cat(selected_features, dim=0)\n",
    "selected_labels = torch.cat(selected_labels, dim=0)\n",
    "print(np.shape(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 2 # approximately ln(1/0.9 - 1)\n",
    "\n",
    "N_reviews = np.shape(selected_features)[0]\n",
    "E = model.state_dict()['encoder.weight'].numpy()\n",
    "v = model.state_dict()['decoder.weight'].numpy()\n",
    "b = model.state_dict()['decoder.bias'].numpy()\n",
    "alpha = model.state_dict()['attention.alpha'].numpy() \n",
    "X = E[selected_features, :] # matrix with the embeddings of all test reviews \n",
    "_, n, d = np.shape(X)\n",
    "s1 = 1 / (1 + alpha)\n",
    "s2 = alpha * s1\n",
    "for rev in range(N_reviews):\n",
    "    z0 = X[rev,:,:].T #initial configuration embedding\n",
    "    ## Run the hardmax dynamics ##\n",
    "    W = np.zeros((d, n))\n",
    "    z = np.zeros((d, n, depth+1)) #we want to do num_steps number of iterations, +1 to save the initial confiiguration\n",
    "    z[:, :, 0] = z0\n",
    "    f = z0.copy()\n",
    "    for iter in range(depth+1):\n",
    "        for i in range(n):\n",
    "            IP = np.dot(f[:, i],f)\n",
    "            Pij = np.zeros(n)\n",
    "            ind = IP == np.max(IP)\n",
    "            Pij[ind] = 1. / np.sum(ind) \n",
    "            W[:, i] =  s2 * np.sum(Pij * f, axis=1)\n",
    "        f = s1 * f + W\n",
    "        z[:, :, iter] = f\n",
    "    # Get all leaders (in the layer = depth, i.e. last layer)\n",
    "    for i in range(n):\n",
    "        y = np.dot(z[:, i, depth], z[:, :, depth])\n",
    "        ind = y == np.max(y)\n",
    "        proj = np.abs(np.dot(z[:, i, depth], v.T) + b)\n",
    "        # proj = np.abs(1 / (1 + np.exp(-proj * proj)) - 0.5)\n",
    "        if i == np.where(ind)[0][0] and proj > tol:\n",
    "            all_leaders.append(int2word[int(selected_features[rev][i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each tag\n",
    "counter = Counter(all_leaders)\n",
    "\n",
    "# Sort the tags by frequency in descending order\n",
    "sorted_data = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select only the most frequent tags\n",
    "top = 15\n",
    "top_data = sorted_data[:top]\n",
    "\n",
    "# Separate the tags and their frequencies\n",
    "tags, frequencies = zip(*top_data)\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.5 \n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(tags, frequencies, width=bar_width, edgecolor='black')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "# Rotate the x-axis labels vertically for better readability\n",
    "plt.xticks(rotation=60, ha='center')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plots_dir + '/histogram_leaders' + '.pdf', format='pdf', dpi=250, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
